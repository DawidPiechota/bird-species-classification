{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bird_species_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZyqBGMLT6_RK",
        "r1gyZYCP7NAn",
        "nAiGIQ-QHyKm",
        "Ed9Sjnc58gqN",
        "cINBfsWfha1C",
        "FDhpAkPHhdWj",
        "p_QHGKoiklw1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyqBGMLT6_RK"
      },
      "source": [
        "# Packages import, dir creation, log functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GNPL7a5521Q"
      },
      "source": [
        "# import packages\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import tarfile\n",
        "import multiprocessing as mp\n",
        "\n",
        "import tqdm\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import sklearn.model_selection as skms\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as td\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision as tv\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "\n",
        "# define constants\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)\n",
        "OUT_DIR = 'results'\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# create an output folder\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def get_model_desc(pretrained=False, num_classes=200, use_attention=False):\n",
        "    \"\"\"\n",
        "    Generates description string.  \n",
        "    \"\"\"\n",
        "    desc = list()\n",
        "\n",
        "    if pretrained:\n",
        "        desc.append('Transfer')\n",
        "    else:\n",
        "        desc.append('Baseline')\n",
        "\n",
        "    if num_classes == 204:\n",
        "        desc.append('Multitask')\n",
        "\n",
        "    if use_attention:\n",
        "        desc.append('Attention')\n",
        "\n",
        "    return '-'.join(desc)\n",
        "\n",
        "\n",
        "def log_accuracy(path_to_csv, desc, acc, sep='\\t', newline='\\n'):\n",
        "    \"\"\"\n",
        "    Logs accuracy into a CSV-file.\n",
        "    \"\"\"\n",
        "    file_exists = os.path.exists(path_to_csv)\n",
        "\n",
        "    mode = 'a'\n",
        "    if not file_exists:\n",
        "        mode += '+'\n",
        "\n",
        "    with open(path_to_csv, mode) as csv:\n",
        "        if not file_exists:\n",
        "            csv.write(f'setup{sep}accuracy{newline}')\n",
        "\n",
        "        csv.write(f'{desc}{sep}{acc}{newline}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1gyZYCP7NAn"
      },
      "source": [
        "# Dataset import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XnfL96zr7MIv",
        "outputId": "3054a28a-16d7-4591-c100-bc56ad57d0c7"
      },
      "source": [
        "class GoogleDriveDownloader(object):\n",
        "    \"\"\"\n",
        "    Downloading a file stored on Google Drive by its URL.\n",
        "    If the link is pointing to another resource, the redirect chain is being expanded.\n",
        "    Returns the output path.\n",
        "    \"\"\"\n",
        "    \n",
        "    base_url = 'https://docs.google.com/uc?export=download'\n",
        "    chunk_size = 32768\n",
        "    \n",
        "    def __init__(self, url, out_dir):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.out_name = url.rsplit('/', 1)[-1]\n",
        "        self.url = self._get_redirect_url(url)\n",
        "        self.out_dir = out_dir\n",
        "    \n",
        "    @staticmethod\n",
        "    def _get_redirect_url(url):\n",
        "        response = requests.get(url)\n",
        "        if response.url != url and response.url is not None:\n",
        "            redirect_url = response.url\n",
        "            return redirect_url\n",
        "        else:\n",
        "            return url\n",
        "    \n",
        "    @staticmethod\n",
        "    def _get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "        return None\n",
        "    \n",
        "    def _save_response_content(self, response):\n",
        "        with open(self.fpath, 'wb') as f:\n",
        "            bar = tqdm.tqdm(total=None)\n",
        "            progress = 0\n",
        "            for chunk in response.iter_content(self.chunk_size):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    progress += len(chunk)\n",
        "                    bar.update(progress - bar.n)\n",
        "            bar.close()\n",
        "    \n",
        "    @property\n",
        "    def file_id(self):\n",
        "        return self.url.split('?')[0].split('/')[-2]\n",
        "    \n",
        "    @property\n",
        "    def fpath(self):\n",
        "        return os.path.join(self.out_dir, self.out_name)\n",
        "    \n",
        "    def download(self):\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "        \n",
        "        if os.path.isfile(self.fpath):\n",
        "            print('File is downloaded yet:', self.fpath)\n",
        "        else:\n",
        "            session = requests.Session()\n",
        "            response = session.get(self.base_url, params={'id': self.file_id}, stream=True)\n",
        "            token = self._get_confirm_token(response)\n",
        "\n",
        "            if token:\n",
        "                response = session.get(self.base_url, params={'id': self.file_id, 'confirm': token}, stream=True)\n",
        "            else:\n",
        "                raise RuntimeError()\n",
        "\n",
        "            self._save_response_content(response)\n",
        "        \n",
        "        return self.fpath\n",
        "\n",
        "\n",
        "# download an archive containing the dataset and store it into the output directory\n",
        "url = 'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz'\n",
        "dl = GoogleDriveDownloader(url, 'data')\n",
        "dl.download()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1150585339it [00:06, 168350795.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data/CUB_200_2011.tgz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DQWcb0j8ODH",
        "outputId": "ecf46807-fd1a-42ca-defe-94b949e5e1de"
      },
      "source": [
        "def extract_tgz(from_path, to_path=None, img_extention='.jpg'):\n",
        "    \"\"\"\n",
        "    Extracts data from '.tgz' file and displays data statistics.\n",
        "    Returns the output directory name.  \n",
        "    \"\"\"\n",
        "    with tarfile.open(from_path, 'r:gz') as tar:   \n",
        "        \n",
        "        if to_path is None:\n",
        "            out_dir = os.path.splitext(from_path)[0]\n",
        "        if os.path.isdir(out_dir):\n",
        "            print('Files are extracted yet.')\n",
        "        else:\n",
        "            print('Extracting files...')\n",
        "        to_path = os.path.dirname(out_dir)\n",
        "\n",
        "        subdir_and_files = [tarinfo for tarinfo in tar.getmembers()]    \n",
        "        imgs = [t for t in subdir_and_files if t.name.endswith(img_extention)]\n",
        "        print('\\tClasses: {}\\n\\tImages: {}'.format(len(set([os.path.dirname(t.name) for t in imgs])), len(imgs)))\n",
        "\n",
        "        tar.extractall(to_path, members=subdir_and_files)\n",
        "        \n",
        "        return out_dir\n",
        "\n",
        "\n",
        "# extract the downloaded archive & assess data statistics\n",
        "in_dir_data = extract_tgz(from_path=dl.fpath)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "\tClasses: 200\n",
            "\tImages: 11788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFKRIh0Y-MFU"
      },
      "source": [
        "in_dir_img = os.path.join(in_dir_data, 'images')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAiGIQ-QHyKm"
      },
      "source": [
        "# Dataset exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed9Sjnc58gqN"
      },
      "source": [
        "## Image corruption check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09gjoJl68kP0",
        "outputId": "c481a056-4252-4399-d1cb-bf7030cd6395"
      },
      "source": [
        "def get_filepaths(path_to_data, fileformat='.jpg'):\n",
        "    \"\"\"\n",
        "    Ruturns paths to files of the specified format.  \n",
        "    \"\"\"             \n",
        "    filepaths = list()\n",
        "    for root, _, finenames in os.walk(path_to_data):\n",
        "        for fn in finenames:\n",
        "            if fn.endswith(fileformat):\n",
        "                filepaths.append(os.path.join(root, fn))\n",
        "                \n",
        "    return filepaths\n",
        "\n",
        "\n",
        "def cleaning_worker(path_to_img):\n",
        "    \"\"\"\n",
        "    Verifies whether the image is corrupted.\n",
        "    \"\"\"\n",
        "    std = np.std(mpimg.imread(path_to_img))\n",
        "    img_ok = not np.isclose(std, 0.0)\n",
        "    \n",
        "    return img_ok, path_to_img\n",
        "\n",
        "\n",
        "# calculate standard deviation of images\n",
        "imgs_corrupted = list()\n",
        "with mp.Pool(processes=mp.cpu_count()) as pool:    \n",
        "    for img_ok, fn in pool.imap_unordered(cleaning_worker, get_filepaths(in_dir_img)):\n",
        "        if not img_ok:\n",
        "            imgs_corrupted.append(fn)\n",
        "\n",
        "# verify do corrupted images (missing data) exist\n",
        "print('Corrupted images #:', len(imgs_corrupted))\n",
        "\n",
        "# clean up the images that aren't OK            \n",
        "# for fn in imgs_corrupted:\n",
        "#    os.remove(fn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corrupted images #: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cINBfsWfha1C"
      },
      "source": [
        "## Similar species"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5hcEuf5a5vr"
      },
      "source": [
        "def plot_simmilar_species(s_name):\n",
        "  img_sparrows = dict()\n",
        "  # get all wróble\n",
        "  sparrows_total = [k for k in os.listdir(in_dir_img) if s_name in k.lower()]\n",
        "  print(\"{} species of {} in dataset\".format(len(sparrows_total), s_name))\n",
        "  ####\n",
        "  some_sparrows = sparrows_total[:5]\n",
        "  for dirname in some_sparrows:\n",
        "      imgs = list()\n",
        "      for dp, _, fn in os.walk(os.path.join(in_dir_img, dirname)):\n",
        "          imgs.extend(fn)\n",
        "      img_sparrows[dirname] = imgs\n",
        "  print(some_sparrows)\n",
        "  ###\n",
        "  row_count = 5\n",
        "  column_count = len(some_sparrows)\n",
        "  f, ax = plt.subplots(row_count, column_count, figsize=(20, 12))\n",
        "  f.patch.set_facecolor('white')\n",
        "\n",
        "  for j in range(row_count):\n",
        "    for i in range(column_count):\n",
        "        cls_name = some_sparrows[i]\n",
        "        img_count = len(img_sparrows[cls_name])\n",
        "        img_name = img_sparrows[cls_name][j % img_count]\n",
        "        path_img = os.path.join(os.path.join(in_dir_img, cls_name), img_name)\n",
        "        ax[j,i].imshow(mpimg.imread(path_img))\n",
        "        if j == 0:\n",
        "          ax[j,i].set_title(cls_name.split('.')[-1].replace('_', ' '),  fontsize=15)\n",
        "        plt.tight_layout()\n",
        "    plt.tight_layout()\n",
        "      \n",
        "  plt.show()\n",
        "\n",
        "# plot_simmilar_species('sparrow')\n",
        "# plot_simmilar_species('auklet')\n",
        "# plot_simmilar_species('blackbird')\n",
        "plot_simmilar_species('hummingbird')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDhpAkPHhdWj"
      },
      "source": [
        "## Size of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVVC9uwYhqRB"
      },
      "source": [
        "# calculate image statistics (takes some time to complete)\n",
        "ds = tv.datasets.ImageFolder(in_dir_img)\n",
        "shapes = [(img.height, img.width) for img, _ in ds]\n",
        "heights, widths = [[h for h,_ in shapes], [w for _,w in shapes]]\n",
        "print('Average sizes:', *map(np.median, zip(*shapes)))\n",
        "\n",
        "# visualize the distribution of the size of images\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "bp = ax.boxplot([heights, widths], patch_artist=True)\n",
        "\n",
        "ax.set_xticklabels(['height', 'width'])\n",
        "ax.set_xlabel('image sizes')\n",
        "ax.set_ylabel('pixels')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_QHGKoiklw1"
      },
      "source": [
        "## Average image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MINhJg5klZd"
      },
      "source": [
        "def pad(img, fill=0, size_max=500):\n",
        "    \"\"\"\n",
        "    Pad image to size_max pixels width and height\n",
        "    \"\"\"\n",
        "    pad_height = max(0, size_max - img.shape[1])\n",
        "    pad_width = max(0, size_max - img.shape[2])\n",
        "    \n",
        "    pad_top = pad_height // 2\n",
        "    pad_bottom = pad_height - pad_top\n",
        "    pad_left = pad_width // 2\n",
        "    pad_right = pad_width - pad_left\n",
        "    \n",
        "    return TF.pad(img, (pad_left, pad_top, pad_right, pad_bottom), fill=fill)\n",
        "\n",
        "\n",
        "ds = tv.datasets.ImageFolder(in_dir_img, transform=tv.transforms.ToTensor())\n",
        "\n",
        "# average image\n",
        "img_mean = np.zeros((3, 500, 500))\n",
        "for img, _ in tqdm.tqdm(ds):\n",
        "    img = pad(img)\n",
        "    img_mean += img.numpy()\n",
        "\n",
        "img_mean = img_mean / len(ds)\n",
        "\n",
        "# plot average image  \n",
        "plt.imshow(np.moveaxis(img_mean, 0, 2))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzevBm___73x"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAYFbGS1_94a"
      },
      "source": [
        "sorted(os.listdir(in_dir_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaHRNHNIAsI5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}